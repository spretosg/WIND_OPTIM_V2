
---
title: "WP_3"
author: "R.Spielhofer"
date: "31/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(raster)
require(rgdal)
require(snow)
require(mapview)
require(dplyr)
require(ggplot2)
require(sf)
require(sp)
require(ecr)
require(Hmisc)
require(tmap)
require(tmaptools)
require(DBI)
require(RPostgres)
require(rpostgis)
require(mapview)
require(maptools)
require(reticulate)
require(spatstat)
require(RPostgreSQL)
require(ggpubr)


#con<- dbConnect(Postgres(), dbname = "publication_3_fin", host = "localhost", user = "postgres", password
 #                       = "reto89LLSIMI")


#st_write(tmp,dsn = con, Id(schema="WT_PU_HEX", table = "CEN_FIN_201008"))


```

###Objective functions
This section defines the objectives as a function of x. While x is a binary vector [0,1] with the length specified in the optimization part. 
```{r helper functions} 

ener_dens<-function(x){
  ener<-sum(x*cen$ENER_DENS)/sum(x)
  return(ener)
}  

#counts the amount of WT in a model run
amount_WT<-function(x){
  am_WT<-sum(x)
  return(am_WT)
}

# The clark evens index (the smaller the value below 0 the more clustered the data is)
cluster_fun<-function(x){
  cen$X<-x
  tmp<-subset(cen,X==1)
  tmp.ppp<-as.ppp.SpatialPoints(tmp)
  ce<-clarkevans(tmp.ppp)
  return(ce[3])
}

```


# The NSGA2 optimization

## general settings
```{r}
#define some empty lists which store the results for each scenario
fitness_list<-list()
ScenName_list<-list()
scen_vec<-vector()

bound<-st_read(dsn= con, Id(schema="GEO_base_data", table = "CH_boundaries"))
bound<-st_transform(bound,crs = "+init=epsg:21781") 

path_last<-"D:/04_PROJECTS/2001_WIND_OPTIM/WIND_OPTIM_git/intermediate_steps/3_wp/OPTIM_RES/fitn_last"
path_arch<-"D:/04_PROJECTS/2001_WIND_OPTIM/WIND_OPTIM_git/intermediate_steps/3_wp/OPTIM_RES/fitn_archive"

#these are the boundaries for the wind energy target of Switzerland
low_targ<-4299000
up_targ<-4400000
low_targ<-1299000
up_targ<-1400000

scen_names<-dbGetQuery(con,"SELECT table_name FROM information_schema.tables WHERE table_schema='scenario'")

#the fitness function calls the three optimization functions. If the energy target is not reached from the individual WT configuration, very high (low) values will be returned
 fitness.fun = function(x){
  if((sum(x*cen$prod_MW)>low_targ)& (sum(x*cen$prod_MW)<up_targ)){
    res<-c(ener_dens(x),amount_WT(x),cluster_fun(x))
  }
  else{res<-c(-10^20,10^20,10^20)}
    return(res)
 }
 
 MU = 80; LAMBDA = MU-40;  MAX.ITER =300
 p.recomb =c(0.5,0.7)
 p.mut = c(0.2,0.4)
 #the baseline points
   #read out the points with the name of the name list at position a
  base<-st_read(dsn = con, Id(schema="scenario", table = "B3_2020-10-21"))
  base<-as_Spatial(base)
 
```


```{r}


#global NSGA2 settings


# Here we loop through all scenario files from the DB and calculate the optimization
for(a in 1:nrow(scen_names)){
  ScenName_list[a]<-as.character(scen_names[a,])

  #read out the points with the name of the name list at position a
  cen<-st_read(dsn = con, Id(schema="scenario", table = as.character(scen_names[a,])))
  cen<-as_Spatial(cen)
  cen<-cen[c(1:300),]
  
  
  #the bounds are calculated to establish an initial population which can reach the energy target (we add +- 10% to    the boundaries)
  bound_up<-up_targ/sum(cen$prod_MW)+0.1*up_targ/sum(cen$prod_MW)
  bound_low<-low_targ/sum(cen$prod_MW)-0.05*low_targ/sum(cen$prod_MW)
  
  
  #we check if there are enough points to reach the energy target of 4.3TWh/y
  if(sum(cen$prod_MW)<low_targ | bound_up>1){
    print(" is not possible to optimize")
    next
    #otherwise proceed with the optimization
  } 
  else{
    print(" will be optimized!!")
    for (n in 1:length(p.mut)){ 
       print(paste(paste(paste( " with p.mut= ", p.mut[n], sep = ""), " and p.recomb =", sep=""), p.recomb[n], sep = ""))
    #cen specific NSGA2 settings
    N.BITS = nrow(cen)
    #control settings
    ctrl<-initECRControl(fitness.fun, n.objectives = 3L, minimize = c(FALSE,TRUE,TRUE))
    ctrl<-registerECROperator(ctrl, "mutate", mutBitflip, p=1/N.BITS)
    ctrl<-registerECROperator(ctrl, "recombine", recCrossover)
    ctrl<-registerECROperator(ctrl, "selectForMating", selSimple)
    ctrl<-registerECROperator(ctrl, "selectForSurvival",  selNondom)
  
    #WE NEED TO DEFINE A VECTOR WITH the lower and the upper bounds of ONES and ZEROS in order to reach the target       specified in the goals
    population<-list()
    for(i in 1:MU){
      x3 <- sample(round(100*bound_low):round(100*bound_up), 1)
      pop_vec<-(sample(1:0, size=nrow(cen), prob=c(x3,100-x3), replace=TRUE))
      population[[i]]<-pop_vec
    }
    #The initial population and it's fitness
    fitness = evaluateFitness(ctrl , population)
  
    #since the compute HV only works for minimized goals, we need to transform the only maximized goal
    fitness2<-fitness
    fitness2[1,]<-fitness2[1,]*-1
   
    ##Setting up the statistics HV   
    ref.point<-c(8,1000,1)
  
    logger<-initLogger(ctrl,log.stats = list(fitness = list("HV"=list(fun=computeHV, pars = list(ref.point=ref.point)))),log.pop=T,init.size = MAX.ITER+1L)
    updateLogger(logger,population=population, fitness = fitness2, n.evals=MU)
  
    arch<-initParetoArchive(ctrl)
    
    ##And here we iterate MAX.iter times through and recombine the individuals to establish optimal solutions for each scenario.
                                             
    start<-Sys.time()
    for(i in seq_len(MAX.ITER)){
      #offspring<-mutate(ctrl, population, p.mut = 0.9)
      offspring<-generateOffspring(ctrl, population, fitness, LAMBDA, p.recomb[n], p.mut[n])
      fitness.o<-evaluateFitness(ctrl,offspring)
      #new selection
      sel<-replaceMuPlusLambda(ctrl, population, offspring, fitness, fitness.o)
      #selected population
      population<-sel$population
      fitness<-sel$fitness
      fitness2<-fitness
      fitness2[1,]<-fitness2[1,]*-1
      
      
      updateLogger(logger, population, fitness = fitness2,n.evals = LAMBDA)
       updateParetoArchive(archive=arch, inds=population, fitness = fitness)
       
       if(i%%100 == 0){
         print(i/MAX.ITER*100)
       }
    }
    end<-Sys.time()
    print(end-start)
  
    stats<-getStatistics(logger)
    
    #save HV curve
    jpeg(file=paste("D:/04_PROJECTS/2001_WIND_OPTIM/WIND_OPTIM_git/intermediate_steps/3_wp/OPTIM_RES/HV_curves/",paste(scen_names[a,], paste (p.recomb[n],p.mut[n],sep="_"),"_HV.jpg",sep=""),sep=""),width = 500, height = 300)
    plot(stats)
    dev.off()
    

    #non_dom population and fitness
    
    nondomPOP<-getIndividuals(arch)
    fitn_arch<-evaluateFitness(ctrl , nondomPOP)
    
     # as soon as the optimization is finished, the extreme populations are extracted (max enerdens, min evens and minimal amount of WT) in order to map these extreme individuals spatially for each scenario  
    enerdens_max<-which.max(fitn_arch[1,])
    enerdens_max<-unlist(nondomPOP[enerdens_max])

    amount_min<-which.min(fitn_arch[2,])
    amount_min<-unlist(nondomPOP[amount_min])

    #the minimal value of clark even index represents the maximal clustering
    clus_max<-which.min(fitn_arch[3,])
    clus_max<-unlist(nondomPOP[clus_max])

    #the extreme individuals are attached to the point layers
    cen$enerdens_max<-enerdens_max
    cen$amount_min<-amount_min
    cen$clus_max<-clus_max
    
    ##transpose for saving as csv
    
    fitn_arch<-t(fitn_arch)
    colnames(fitn_arch)<-c("mean_MWH/yHA","number_WT","CE_index")
    
    
    nondom_fit_last<-which(dominated(fitness)==F)
    nondom_fit_last<-t(fitness[1:3, nondom_fit_last])
    colnames(nondom_fit_last)<-c("mean_MWH/yHA","number_WT","CE_index")
    
    write.csv(fitn_arch,paste(path_arch,paste(scen_names[a,], paste (paste(p.recomb[n],p.mut[n],sep="_"),"csv",sep="."),sep="_"), sep = "/"))
    write.csv(nondom_fit_last,paste(path_last,paste(scen_names[a,], paste (paste(p.recomb[n],p.mut[n],sep="_"),"csv",sep="."),sep="_"), sep = "/"))

    #calculate how often each site is in the pareto optimal in order to get a feeling about the confidence of the optimal sites
    
    b<-Reduce(`+`, nondomPOP)/length(nondomPOP)


    
    cen$tmp<-b
    names(cen)[names(cen) == "tmp"]<-as.character(paste("rel_arch_nonDOM",paste (p.recomb[n],p.mut[n],sep="_"),sep="_"))
    
    print ("next parameter setting")
    rm(fitn_arch, logger, arch)
    }
    cen$delta<-unlist(cen@data[42]-cen@data[43])
    n_sec<-nrow(subset(cen,cen$delta<0.1&cen$delta>0.1*-1))/nrow(cen)*100
    
    cen<-st_as_sf(cen)
    ## map the uncertainties and save it!
 map_tmp<-ggplot(cen) +
    geom_sf(data = bound, fill = NA, color = "gray", size=.6)+
    geom_sf(data = cen, aes(color= delta), show.legend = "point", size=1)+
   scale_color_viridis_c()+
    ggtitle(paste(scen_names[a,], paste (p.recomb[n],p.mut[n],sep="_"),"_securityMAP",sep=""), subtitle = paste(n_sec,"% of all WT's are very similar (+-10%) to both parameter settings",sep=""))+
    theme_light()+
   theme(axis.text=element_text(size=10))

jpeg(file=paste("D:/04_PROJECTS/2001_WIND_OPTIM/WIND_OPTIM_git/intermediate_steps/3_wp/OPTIM_RES/maps/sec_maps/",paste(scen_names[a,], paste (p.recomb[n],p.mut[n],sep="_"),"_map.jpg",sep=""),sep=""),width = 1200, height = 1200,res=300)
plot(map_tmp)
dev.off()
  
    #store cen in the DB with the scenario NAME
    
    st_write(obj = cen, dsn = con, Id(schema="optim_res_201022", table = as.character(paste(scen_names[a,],"_sens_res",sep=""))))
 

    print(paste(paste(paste(a, " of total ",sep = ""), nrow(scen_names), sep=""), " scenarios are computed", sep = ""))
    #clean this for the next run
  } 
}

```
##ecr wrapper



##NSGA-3
In python with deap. The function optim refers to a Python script which optimizes the input data with the NSGA3. The output are the pareto fitness, the data logger, the fitness of the last iteration, the relative robustness of the pareto and the last run population.

```{r}
#for the arcgis approach

main_path<-"D:/04_PROJECTS/2001_WIND_OPTIM/WIND_OPTIM_git/intermediate_steps/3_wp/NSGA3_RES"
in_path<-paste(main_path,"in",sep="/")
out_path<-paste(main_path,"out",sep="/")


Sys.setenv(RETICULATE_PYTHON = "C:/Python27/ArcGISx6410.7/python.exe")
use_python("C:/Python27/ArcGISx6410.7/python.exe",required = TRUE)
source_python("D:/04_PROJECTS/2001_WIND_OPTIM/NSGA3/NSGA3_201210.py")
cur_dat<-Sys.Date()
run_path<-paste(out_path,paste("run",cur_dat,sep = "_"),sep = "/")
dir.create(run_path)
  
#check the files in the input directory
file.names <- dir(in_path, pattern =".shp")
## how many iterations (generations) and how big is the population?
NGEN = 10000
MU = 220
 # CXPB  is the probability with which two individuals are crossed
# MUTPB is the probability for mutating an individual
  CXPB = 0.5
  MUTPB = 0.2


for(n in 1:length(file.names)){
  starttime<-Sys.time()
  
  tmp_name<-as.character(file.names[n])
  tmp_path<-paste(in_path,tmp_name,sep = "/")
  tmp_name<-substr(tmp_name,1,nchar(tmp_name)-4)
  tmp_pts = st_read(tmp_path)
  ##here we calculate the 0,1 vector with just the fewest WT to reach 4.3 TW
  pts<-tmp_pts[order(-tmp_pts$prod_MW),]
  pts$vec<-rep(0,nrow(pts))
  sum_prod = 0
  ind=0

  for(i in 1:nrow(pts)){
    if(sum_prod>4300000){
    break
    }
  tmp_prod<-pts$prod_MW[i]
  sum_prod<-sum_prod+tmp_prod
  pts$vec[i] <- 1
  }
  #to achieve original order
  pts<-pts[order(pts$WT_ID),]
  
  A<-r_to_py(pts$vec)
  
  ### and the max enerdens
  pts<-tmp_pts[order(-tmp_pts$ENER_DENS),]
  pts$vec<-rep(0,nrow(pts))
  sum_prod = 0
  ind=0

  for(i in 1:nrow(pts)){
    if(sum_prod>4300000){
    break
    }
  tmp_prod<-pts$prod_MW[i]
  sum_prod<-sum_prod+tmp_prod
  pts$vec[i] <- 1
  }
  #to achieve original order
  pts<-pts[order(pts$WT_ID),]
  B<-pts$vec
  B<-r_to_py(B)
  
  #transform the tmp_pts into spatial
  tmp_pts<-as_Spatial(tmp_pts)
  ####### THE python optimization
 
  a<-optim(as.integer(MU), as.integer(NGEN), as.character(tmp_path), CXPB, MUTPB, A, B)
  
  ### Extract the HV values for each generation
  hv<-as.data.frame(unlist(a[1]))
  hv$GENERATION<-c(1:NGEN)
  colnames(hv) <- c("HV","GENERATION")
  ## draw a graph
  HV<-ggplot(data=hv, aes(x=GENERATION, y=HV, group=1)) +
  geom_line()+
  ggtitle(paste(tmp_name, paste ("_HV",cur_dat,sep="_")))
  jpeg(file=paste(run_path,paste("HV_graph",paste(tmp_name,"jpg",sep="."),sep="_"),sep="/"))
  plot(HV)
  dev.off()
  
  
  ### extract the pareto robustness
  par_rob<-as.data.frame(a[2])
  #join it to the spatial pts
  #df<-left_join(tmp_pts@data, par_rob, by=c('WT_ID'='WT_ID2'))
  tmp_pts$par_rob<-par_rob$par_rob
  tmp_pts$WT_ID2<-par_rob$WT_ID2

  tmp_pts<-st_as_sf(tmp_pts)
  #write the pts into the postgre sql db
  #st_write(obj = tmp_pts, dsn = con, Id(schema="NSGA3_out", table = as.character(paste(tmp_name,cur_dat,sep="_"))))
  #write pts as shp in folder
  st_write(tmp_pts,paste(run_path,paste(tmp_name,"shp",sep="."),sep = "/"))
  
  ### extract the pareto fitness
  par_fit<-as.data.frame(a[3])
  #and save it
  write.csv(par_fit,paste(run_path,paste("par_fitness",paste(tmp_name,paste(cur_dat,"csv",sep="."),sep="_"),sep="_"),sep="/"))
  
  log<-as.data.frame(a[4])
  ###extract the logbook
  write.csv(log,paste(run_path,paste("logbook",paste(tmp_name,paste(cur_dat,"csv",sep="."),sep="_"),sep="_"),sep="/"))
  
  #graph of the three goals from the data logger
  WT<-ggplot(data=log, aes(x=GENERATION, y=N_WT, group=1)) +
  geom_line()
  
  CLUS<-ggplot(data=log, aes(x=GENERATION, y=CLUS, group=1)) +
  geom_line()
   
  ENER<-ggplot(data=log, aes(x=GENERATION, y=ENERDENS, group=1)) +
  geom_line()
  
  graph_path<-paste(run_path,paste("log_graphs",cur_dat,sep="_"),sep="/")
  tmp_plt<-ggarrange(WT,CLUS,ENER,3,1,c("min_NWT","max_clus","max_enerdens"))
  jpeg(file=paste(run_path,paste("log_graphs",paste(tmp_name,"jpg",sep="."),sep="_"),sep="/"))
  plot(tmp_plt)
  dev.off()
  
  endtime<-Sys.time()
  calcTime<-endtime-starttime
  print("------------------------------next scenario------------------------------------------")
}

#write some parameters into df
stats<-data.frame(matrix(1,0,5))
names(stats)<-c("N-scen","NGEN","MU","CXPB","MUTPB")
stats[1,]<-c(as.integer(length(file.names)),as.integer(NGEN),as.integer(MU),CXPB,MUTPB)

write.csv(stats,file = paste(run_path,"parameters.csv",sep="/"))

```


```{python NSGA3-test, echo=TRUE, engine.path="C:/Python27/ArcGISx6410.7/python.exe"}
# Import system modules
import arcpy
import numpy as np
from deap import base
from deap import creator
from deap import tools
import matplotlib.pyplot as plt

import random
import time



# load the shp of the scenario
all_pts = "D:/04_PROJECTS/2001_WIND_OPTIM/B1_tmp.shp"
#transform it to numpy array
na = arcpy.da.TableToNumPyArray(all_pts, ['WT_ID', 'ENER_DENS', 'prod_MW'])

#some parameters to define the random individual
nBITS = len(na)

#production of energy
sum_MW = np.sum(na['prod_MW'])
low_targ = 4290000
up_targ = low_targ*1.06
# relative to the total energy production to build the initial vector
bound_up = (1.0 * up_targ / sum_MW)
bound_low = (1.0*low_targ / sum_MW)

#global NSGA3 param
MU = 5
NGEN = 5


#the function to determine the initial random population which might reach the energy target
def initial_indi():
 x3 = random.uniform(bound_low, bound_up)
 return np.random.choice([1, 0], size=(nBITS,), p=[x3, 1-x3])

#some lists for the evaluation function
enerd = list(na['ENER_DENS'])
prod = list(na['prod_MW'])
id = np.array(na['WT_ID'])

#the evaluation function, taking the individual vector as input

def evaluate(individual):
  individual = individual[0]
  #first check if the production of the seleced WT's is in the range between 4.31 and 4.29 TWH
  # goal 1
  mean_enerdsel = sum(x * y for x, y in zip(enerd, individual)) / sum(individual)
  # goal 2
  count_WTsel = sum(individual)
  # goal 3 (subset the input points by the WT_IDs which are in the ini pop (=1)
  WT_pop = np.column_stack((id, individual))
  WT_sel = WT_pop[WT_pop[:, [1]] == 1]
  WT_sel = WT_sel.astype(int)
  qry = '"WT_ID" IN ' + str(tuple(WT_sel))
  subset = arcpy.MakeFeatureLayer_management(all_pts, "tmp", qry)
  nn_output = arcpy.AverageNearestNeighbor_stats(subset, "EUCLIDEAN_DISTANCE", "NO_REPORT", "41290790000")
  clus = float(nn_output.getOutput(0))
  res = (clus, count_WTsel, mean_enerdsel)
  ## delete the feature tmp since otherwise it will not work in a loop
  arcpy.Delete_management("tmp")
  arcpy.Delete_management("subset")
  return(res)

def feasible (individual):
    individual = individual[0]
    prod_MWsel = sum(x * y for x, y in zip(prod, individual))
    if (prod_MWsel <= up_targ and prod_MWsel >= low_targ):
        return True
    return False


### setup NSGA3 with deap (minimize the first two goals returned by the evaluate function and maximize the third one)
creator.create("FitnessMulti", base.Fitness, weights=(-1.0, -1.0, 1.0))
creator.create("Individual", list, fitness=creator.FitnessMulti)

#??
ref_points = tools.uniform_reference_points(nobj=3, p=12)
##setup the optim toolbox I do not understand that totally
toolbox = base.Toolbox()

#initial individual and pop
toolbox.register("initial_indi", initial_indi)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.initial_indi, n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

#evaluation and constraints
toolbox.register("evaluate", evaluate)

##assign the feasibility of solutions and if not feasible a large number for the minimization tasks and a small number for the maximization task
toolbox.decorate("evaluate", tools.DeltaPenalty(feasible, (10e20, 10e20, 0)))

#mate, mutate and select to perform crossover
toolbox.register("mate", tools.cxTwoPoint)
#toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)
toolbox.register("mutate", tools.mutPolynomialBounded,  low=0, up=1, eta=20, indpb=0.05)
toolbox.register("select", tools.selNSGA3, ref_points=ref_points)

## the optimization (n seems to be the number of individuals aka population size MU

# initialize pareto front
pareto = tools.ParetoFront(similar=np.array_equal)
#hof = tools.HallOfFame(1)
    # Initialize statistics object
#stats = tools.Statistics(lambda ind: ind.fitness.values)
#stats.register("test", np.mean)
#stats.register("test2", np.mean, axis=1)
#stats.register("test3", np.mean, axis=2)
#stats.register("std", np.std, axis=1)
#stats.register("min", np.min, axis=1)
#stats.register("max", np.max, axis=1)

# first_stats = tools.Statistics(key=lambda ind: ind.fitness.values[0])
stats = tools.Statistics(key=lambda ind: ind.fitness.values[1])
#third_stats = tools.Statistics(key=lambda ind: ind.fitness.values[2])
#stats = tools.MultiStatistics(clus=first_stats)
stats.register("min_WT", np.min, axis=0)
#stats.register("max", np.max, axis=0)


logbook = tools.Logbook()
#logbook.header = "gen", "evals", "std", "min", "avg", "max"
logbook.header = "gen", "evals", "min_WT"
pop = toolbox.population(n=MU)
# Evaluate the individuals with an invalid fitness
invalid_ind = [ind for ind in pop if not ind.fitness.valid]
# invalid_ind = pop
fitnesses = list(toolbox.map(toolbox.evaluate, invalid_ind))
for ind, fit in zip(invalid_ind, fitnesses):
    ind.fitness.values = fit

# Compile statistics about the population
record = stats.compile(pop)
logbook.record(gen=0, evals=len(invalid_ind), **record)


# CXPB  is the probability with which two individuals
#       are crossed
# MUTPB is the probability for mutating an individual
CXPB, MUTPB = 0.7, 0.4


# Begin the evolution with NGEN repetitions
for gen in range(1, NGEN):
    print("-- Generation %i --" % gen)
    start_time = time.time()
    offspring = toolbox.select(pop, len(pop))
    # Clone the selected individuals
    offspring = list(map(toolbox.clone, offspring))

    # Apply crossover and mutation on the offspring
    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if random.random() < CXPB:
            toolbox.mate(child1[0], child2[0])
            del child1.fitness.values
            del child2.fitness.values

    for mutant in offspring:
        if random.random() < MUTPB:
            toolbox.mutate(mutant[0])
            del mutant.fitness.values

    # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]

    fitnesses = list(toolbox.map(toolbox.evaluate, invalid_ind))
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit


#select the next generation with NSGA3 from pop and offspring of size MU
    pop = toolbox.select(pop + offspring, MU)


    pareto.update(pop)

    # Compile statistics about the new population
    record = stats.compile(invalid_ind)
    logbook.record(gen=gen, evals=len(invalid_ind), **record)
    print(logbook.stream)
    print("--- %s seconds ---" % (time.time() - start_time))

#plt.plot(logbook.select('gen'), logbook.select('min_WT'))
#plt.show()


```

```{r}
ggplot(py$logbook , aes(carrier, arr_delay)) + geom_point() + geom_jitter()
```

